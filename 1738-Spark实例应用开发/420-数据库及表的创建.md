# 2 Spark 访问 MySQL 实例

## 2.1 课程介绍

本课程将介绍如何通过 jdbc 的方式在 Spark SQL 中访问外部数据源。

### 2.1.1实验环境

当前实验的系统和软件环境如下：

* Ubuntu 16.04.6 LTS
* SequoiaDB version: 5.0
* SequoiaSQL-MySQL version: 3.4
* openjdk version "1.8.0_242"
* IntelliJ IDEA Community Version: 2019.3.4
* spark version: 2.4.3

## 2.2 知识点

在 Spark SQL 中支持通用的 SQL 方式关联外部数据源，只要在 Spark 中创建了外部数据源的关联表，就可以按照 标准 jdbc 的方式通过 Spark 访问外部数据源。以本实验中使用的 employee 表为例：

<img src="https://raw.githubusercontent.com/AarZK/picstore/master/20200406170227.png" alt="image-20200405145643902" style="zoom:80%;" />

在 Spark 中创建关联表只需要指定数据源，无需定义表结构，在关联时 Spark 会根据结构化数据源自动生成表结构。绑定外部数据源时使用到的参数说明如下：

```sql
CREATE TABLE sample.employee
USING org.apache.spark.sql.jdbc      -- 使用Spark jdbc驱动创建关联表
OPTIONS (
url 'jdbc:mysql://localhost:3306',   -- jdbc url
dbtable "sample.employee",           -- 数据源的库名以及表名
user 'root',                         -- 数据源用户名 
password 'root',                     -- 数据源密码
driver 'com.mysql.jdbc.Driver'       -- 数据源的jdbc驱动
);
```

## 2.3 实验步骤

### 2.3.1 Maven 工程介绍

* 打开 SCDD-Spark 工程

* ![image-20200405155947877](https://raw.githubusercontent.com/AarZK/picstore/master/20200406170235.png)

* 当前实验使用到的 Maven 依赖

  ```xml
          <dependency>
              <!-- hive 的 jdbc 连接依赖 -->
              <groupId>org.apache.hive</groupId>
              <artifactId>hive-jdbc</artifactId>
              <version>1.2.1</version>
          </dependency>
  
          <dependency>
              <!-- hadoop 通用依赖 -->
              <groupId>org.apache.hadoop</groupId>
              <artifactId>hadoop-common</artifactId>
              <version>2.4.1</version>
          </dependency>
  
          <dependency>
              <!-- mysql jdbc 驱动 -->
              <groupId>mysql</groupId>
              <artifactId>mysql-connector-java</artifactId>
              <version>5.1.47</version>
          </dependency>
  
          <dependency>
              <!-- 解析json -->
              <groupId>com.alibaba</groupId>
              <artifactId>fastjson</artifactId>
              <version>1.2.58</version>
          </dependency>
  
          <dependency>
              <!-- 使用Script Runner执行sql脚本 -->
              <groupId>com.ibatis</groupId>
              <artifactId>ibatis2-common</artifactId>
              <version>2.1.7.597</version>
          </dependency>
  ```

* 打开当前实验所在包

  ![image-20200404210526454](https://raw.githubusercontent.com/AarZK/picstore/master/20200406170228.png)

### 2.3.2 初始化源表数据

* 运行源表初始化程序

  ![image-20200405152639310](https://raw.githubusercontent.com/AarZK/picstore/master/20200406170229.png)

* 程序运行结果如下，可以看到初始化的employee表结构及数据内容

* ![image-20200405155603299](https://raw.githubusercontent.com/AarZK/picstore/master/20200406170233.png)

### 2.3.3 关联MySQL实例

> **说明**
>
> 程序中会调用到第一章中定义的 jdbc 工具类方法

* step1：在 Spark 中建表并和 MySQL 实例中的 employee 关联

  ```java
          // 初始化表
          String dropTable =
                  "DROP TABLE\n" +
                          "IF\n" +
                          "\tEXISTS employee";
          // 调用HiveUtil的doDDL()方法初始化employee表
          HiveUtil.doDDL(dropTable);
          // 创建Spark表（将MySQL实例表作为外部数据源）
          String createLinkTable =
                  "CREATE TABLE employee\n" +
                          "USING org.apache.spark.sql.jdbc\n" +
                          "OPTIONS (\n" +
                          "url 'jdbc:mysql://localhost:3306',\n" +
                          "dbtable \"sample.employee\",\n" +
                          "user 'root',\n" +
                          "password 'root',\n" +
                          "driver 'com.mysql.jdbc.Driver'\n" +
                          ")";
          // 调用HiveUtil的doDDL()方法在Spark中创建MySQL实例employee表的关联外表
          HiveUtil.doDDL(createLinkTable);
  ```

  将上述代码粘贴至 `!TODO -- lesson2_createtable:step1` 标签处

  ![image-20200405153052567](https://raw.githubusercontent.com/AarZK/picstore/master/20200406170230.png)

* step2：打印关联表的表结构和记录内容

  ```java
          // 查看Spark表结构
          String getDesc=
                  "desc employee";
          // 调用HiveUtil的doDQL()方法查询表结构
          ResultSet resultSet1=HiveUtil.doDQL(getDesc);
          try {
              // 打印查询表结构结果集
              ResultFormat.printResultSet(resultSet1);
          } catch (SQLException e) {
              e.printStackTrace();
          }
          // 查询Spark表数据
          String queryTable = "select id,name,sex,birth,phone,email,position,address from employee";
          // 调用HiveUtil的doDQL()方法查询Spark表数据
          ResultSet resultSet2=HiveUtil.doDQL(queryTable);
          try {
              // 打印查询结果集
              ResultFormat.printResultSet(resultSet2);
          } catch (SQLException e) {
              e.printStackTrace();
          }
  ```

  将上述代码粘贴至 `!TODO -- lesson2_createtable:step2` 标签处

  ![image-20200405153345685](https://raw.githubusercontent.com/AarZK/picstore/master/20200406170231.png)

### 2.3.4 运行程序

* 右键点击 Run 运行程序

  ![image-20200405155803666](https://raw.githubusercontent.com/AarZK/picstore/master/20200406170234.png)

* 运行结果如下：

  <img src="https://raw.githubusercontent.com/AarZK/picstore/master/20200406170232.png" alt="image-20200405155328063"  />

  可以看到关联后 Spark 自动匹配了 MySQL 实例中表字段的类型，并且可以通过 Spark 直接查询在 2.3.2 中初始化的 MySQL 实例数据。

## 2.4 总结

经过本课程学习，我们可以通过 Spark SQL 创建和 MySQL 实例关联的外表，并可以通过 jdbc 的方式访问 Spark 的外部数据源。
